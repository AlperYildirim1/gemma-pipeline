{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/gemma-pipeline/blob/main/makale_gemma_3_4b_test_pipeline_open_ended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWNWiijSUDQ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZzL79PJXGGA",
        "outputId": "314df50c-c04f-4287-dc76-7b9e5e92540a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q-4GfMASUDQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCTzRCNlW7_-"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: CONFIGURATION\n",
        "# ==============================================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastModel\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Main Configuration ---\n",
        "# Set this to the SFT or GRPO model you want to test\n",
        "MODEL_NAME = \"Yujivus/gemma-3_sft_grpo\"\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "JUDGE_MODEL = \"gpt-4.1-2025-04-14\"\n",
        "\n",
        "# New file paths for the Verifiable Problems benchmark results\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/gemma sft cevaplar\"\n",
        "FINAL_RESULTS_JSON = os.path.join(OUTPUT_DIR, \"final_evaluation_results_verifiable_problems_grpo_model_4b.json\")\n",
        "INTERMEDIATE_CSV = os.path.join(OUTPUT_DIR, \"intermediate_generated_answers_verifiable_problems_grpo_model_4b.csv\")\n",
        "\n",
        "# Set to 100 as requested\n",
        "MAX_SAMPLES = 100\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded. Model: {MODEL_NAME}, Benchmark: Verifiable Problems\")\n",
        "print(f\"Final output will be saved to: {FINAL_RESULTS_JSON}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: CREATE DATASET AND LOAD MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Loading source datasets to create the test set...\")\n",
        "verifiable_dataset = load_dataset(\"FreedomIntelligence/medical-o1-verifiable-problem\", split=\"train\")\n",
        "sft_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", 'en', split=\"train\")\n",
        "\n",
        "print(\"Creating a set of questions used in the SFT dataset...\")\n",
        "sft_questions_set = set(item['Question'] for item in tqdm(sft_dataset, desc=\"Processing SFT questions\"))\n",
        "\n",
        "print(\"Filtering verifiable problems to create the unseen RL/Test dataset...\")\n",
        "rl_dataset = []\n",
        "for item in tqdm(verifiable_dataset, desc=\"Filtering for Test Set\"):\n",
        "    verifiable_question = item['Open-ended Verifiable Question']\n",
        "    if verifiable_question not in sft_questions_set:\n",
        "        rl_dataset.append({\n",
        "            \"question\": verifiable_question,\n",
        "            \"ground_truth_answer\": item['Ground-True Answer']\n",
        "        })\n",
        "print(f\"Created a test set with {len(rl_dataset)} unseen problems.\")\n",
        "\n",
        "# --- Part 2: Preparing the final test_data variable ---\n",
        "if MAX_SAMPLES is not None:\n",
        "    print(f\"Selecting the first {MAX_SAMPLES} samples for this run.\")\n",
        "    test_data = rl_dataset[:MAX_SAMPLES]\n",
        "else:\n",
        "    test_data = rl_dataset\n",
        "\n",
        "print(f\"âœ… Dataset ready. Using {len(test_data)} samples for evaluation.\")"
      ],
      "metadata": {
        "id": "TAE-zAnrFblQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VkH60sdYDMV"
      },
      "outputs": [],
      "source": [
        "# --- Part 3: Loading the model in full precision ---\n",
        "print(\"\\nLoading model and tokenizer with Unsloth...\")\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        ")\n",
        "print(\"âœ… Model loaded successfully in full precision.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q6QRbFIX9dN"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: PHASE 1 - GENERATE ANSWERS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 1: Generating Answers ---\")\n",
        "\n",
        "generated_results = []\n",
        "\n",
        "# Loop through the prepared test data\n",
        "for idx, item in enumerate(tqdm(test_data, desc=\"Generating Answers\")):\n",
        "    question_text = item['question']\n",
        "    ground_truth_answer = item['ground_truth_answer']\n",
        "\n",
        "    # Simple prompt for an open-ended question\n",
        "    user_prompt = f\"Please answer the following question:\\n\\n{question_text}\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the model's response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        # Using 4096 tokens to allow the CoT model to generate its full reasoning.\n",
        "        max_new_tokens=2048,\n",
        "        # Using temp=0.0 for a deterministic, reproducible benchmark.\n",
        "        temperature=0.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    model_full_answer = full_response.split(user_prompt)[-1].strip()\n",
        "\n",
        "    # Saving the relevant columns\n",
        "    generated_results.append({\n",
        "        \"question_number\": idx,\n",
        "        \"question\": question_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"model_full_answer\": model_full_answer,\n",
        "    })\n",
        "\n",
        "# Save intermediate results to a CSV file\n",
        "df_answers = pd.DataFrame(generated_results)\n",
        "df_answers.to_csv(INTERMEDIATE_CSV, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Phase 1 Complete. {len(generated_results)} answers saved to {INTERMEDIATE_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_92PvLZ-b5a7"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: PHASE 2 - EVALUATE WITH LLM-AS-JUDGE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 2: Evaluating and Combining Results ---\")\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\") == \"\":\n",
        "    print(\"ðŸš¨ WARNING: OpenAI API key is not set. Skipping Phase 2.\")\n",
        "else:\n",
        "    try:\n",
        "        df_to_judge = pd.read_csv(INTERMEDIATE_CSV)\n",
        "        client = OpenAI()\n",
        "        final_results_list = []\n",
        "\n",
        "        # Judge prompt for open-ended medical answers, allowing for synonyms.\n",
        "        judge_system_prompt = \"\"\"You are an expert medical evaluator. Your task is to evaluate a model's response to a medical question against a ground-truth answer. You will assess two distinct aspects: the correctness of the final conclusion and the soundness of the reasoning that led to it.\n",
        "\n",
        "The question is open-ended.\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "\n",
        "**1. Final Answer Correctness (for the `correctness` field):**\n",
        "- Determine if the model's **final conclusion** is a correct and synonymous match for the ground-truth answer.\n",
        "- Be lenient with synonyms. For example, 'Gastric ulcer' and 'Stomach ulcer' are correct matches.\n",
        "- Focus on medical accuracy. If the model names a different condition, it is incorrect.\n",
        "\n",
        "**2. Reasoning Soundness (for the `reasoning_correctness` field):**\n",
        "- Evaluate the model's Chain-of-Thought (CoT) or reasoning steps.\n",
        "- The reasoning is considered **correct** if it is medically sound, logical, and does not contain significant errors. Brief but accurate reasoning is acceptable.\n",
        "- The reasoning is considered **incorrect** if it contains significant medical inaccuracies or logical fallacies, **even if the final answer happens to be correct.**\n",
        "\n",
        "**Output Format:**\n",
        "\n",
        "You MUST respond ONLY with a valid JSON object with the following structure:\n",
        "{\n",
        "  \"correctness\": boolean,\n",
        "  \"reasoning_correctness\": boolean,\n",
        "  \"justification\": \"A brief explanation for your decisions on both correctness and reasoning_correctness. Note any synonyms used or specific flaws in the reasoning.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "        for _, row in tqdm(df_to_judge.iterrows(), total=len(df_to_judge), desc=\"Judging Answers\"):\n",
        "            final_record = row.to_dict()\n",
        "            # The user prompt for the judge does not need the reasoning correctness part anymore\n",
        "            judge_user_prompt = f\"\"\"\n",
        "Please evaluate the following model output.\n",
        "\n",
        "**Original Question:**\n",
        "{row['question']}\n",
        "\n",
        "**Ground Truth Answer:**\n",
        "{row['ground_truth_answer']}\n",
        "\n",
        "**Model's Full Response (including reasoning):**\n",
        "{row['model_full_answer']}\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=JUDGE_MODEL,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.0,\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "                judge_assessment = json.loads(response.choices[0].message.content)\n",
        "                final_record['judge_evaluation'] = judge_assessment\n",
        "            except Exception as e:\n",
        "                print(f\"Error judging question {row['question_number']}: {e}\")\n",
        "                final_record['judge_evaluation'] = {\"correctness\": \"error\", \"justification\": str(e)}\n",
        "            final_results_list.append(final_record)\n",
        "\n",
        "        with open(FINAL_RESULTS_JSON, 'w') as f:\n",
        "            json.dump(final_results_list, f, indent=4)\n",
        "        print(f\"\\nâœ… Phase 2 Complete. Final results saved to {FINAL_RESULTS_JSON}\")\n",
        "\n",
        "        correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('correctness') is True)\n",
        "        total_judged = len(final_results_list)\n",
        "\n",
        "        if total_judged > 0:\n",
        "            accuracy = (correct_count / total_judged) * 100\n",
        "            print(\"\\n--- Final Verifiable Problems Results ---\")\n",
        "            print(f\"Answer Correctness: {accuracy:.2f}% ({correct_count}/{total_judged})\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ðŸš¨ ERROR: The intermediate answers file was not found at {INTERMEDIATE_CSV}. Cannot run Phase 2.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸš¨ An unexpected error occurred during Phase 2: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy5_DpGLtwPa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: PHASE 2 - EVALUATE WITH LLM-AS-JUDGE AND COMBINE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 2: Evaluating and Combining Results ---\")\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\") == \"sk-...\":\n",
        "    print(\"ðŸš¨ WARNING: OpenAI API key is not set. Skipping Phase 2.\")\n",
        "else:\n",
        "    try:\n",
        "        # Load the generated answers\n",
        "        df_to_judge = pd.read_csv(INTERMEDIATE_CSV)\n",
        "        client = OpenAI()\n",
        "\n",
        "        # REVISED: This list will hold the final combined data\n",
        "        final_results_list = []\n",
        "\n",
        "        judge_system_prompt = \"\"\"You are an expert medical evaluator. Your task is to evaluate a model's response to a medical question against a ground-truth answer. You will assess two distinct aspects: the correctness of the final conclusion and the soundness of the reasoning that led to it.\n",
        "\n",
        "The question is open-ended.\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "\n",
        "**1. Final Answer Correctness (for the `correctness` field):**\n",
        "- Determine if the model's **final conclusion** is a correct and synonymous match for the ground-truth answer.\n",
        "- Be lenient with synonyms. For example, 'Gastric ulcer' and 'Stomach ulcer' are correct matches.\n",
        "- Focus on medical accuracy. If the model names a different condition, it is incorrect.\n",
        "\n",
        "**2. Reasoning Soundness (for the `reasoning_correctness` field):**\n",
        "- Evaluate the model's Chain-of-Thought (CoT) or reasoning steps.\n",
        "- The reasoning is considered **correct** if it is medically sound, logical, and does not contain significant errors. Brief but accurate reasoning is acceptable.\n",
        "- The reasoning is considered **incorrect** if it contains significant medical inaccuracies or logical fallacies, **even if the final answer happens to be correct.**\n",
        "\n",
        "**Output Format:**\n",
        "\n",
        "You MUST respond ONLY with a valid JSON object with the following structure:\n",
        "{\n",
        "  \"correctness\": boolean,\n",
        "  \"reasoning_correctness\": boolean,\n",
        "  \"justification\": \"A brief explanation for your decisions on both correctness and reasoning_correctness. Note any synonyms used or specific flaws in the reasoning.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "        for _, row in tqdm(df_to_judge.iterrows(), total=len(df_to_judge), desc=\"Judging Answers\"):\n",
        "            # REVISED: Start building the final record for this item\n",
        "            final_record = row.to_dict()\n",
        "\n",
        "            judge_user_prompt = f\"\"\"\n",
        "Please evaluate the following model output.\n",
        "\n",
        "**Original Question:**\n",
        "{row['question']}\n",
        "\n",
        "**Ground Truth Answer:**\n",
        "{row['ground_truth_answer']}\n",
        "\n",
        "**Model's Full Response (including reasoning):**\n",
        "{row['model_full_answer']}\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=JUDGE_MODEL,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.0, # Judge should be deterministic\n",
        "                    response_format={\"type\": \"json_object\"} # Enforce JSON output\n",
        "                )\n",
        "\n",
        "                judge_assessment = json.loads(response.choices[0].message.content)\n",
        "                # REVISED: Add the judge's assessment to the final record\n",
        "                final_record['judge_evaluation'] = judge_assessment\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error judging question {row['question_number']}: {e}\")\n",
        "                final_record['judge_evaluation'] = {\n",
        "                    \"correctness\": \"error\",\n",
        "                    \"reasoning_correctness\": \"error\",\n",
        "                    \"justification\": str(e)\n",
        "                }\n",
        "\n",
        "            # REVISED: Add the complete record to our final list\n",
        "            final_results_list.append(final_record)\n",
        "\n",
        "        # REVISED: Save the final combined list to a single JSON file\n",
        "        with open(FINAL_RESULTS_JSON, 'w') as f:\n",
        "            json.dump(final_results_list, f, indent=4)\n",
        "\n",
        "        print(f\"\\nâœ… Phase 2 Complete. Final combined results for {len(final_results_list)} items saved to {FINAL_RESULTS_JSON}\")\n",
        "\n",
        "        # Final Summary\n",
        "        correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('correctness') is True)\n",
        "        reasoning_correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('reasoning_correctness') is True)\n",
        "        total_judged = len(final_results_list)\n",
        "\n",
        "        if total_judged > 0:\n",
        "            accuracy = (correct_count / total_judged) * 100\n",
        "            reasoning_accuracy = (reasoning_correct_count / total_judged) * 100\n",
        "            print(\"\\n--- Final Results ---\")\n",
        "            print(f\"Answer Correctness: {accuracy:.2f}% ({correct_count}/{total_judged})\")\n",
        "            print(f\"Reasoning Correctness: {reasoning_accuracy:.2f}% ({reasoning_correct_count}/{total_judged})\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ðŸš¨ ERROR: The intermediate answers file was not found at {INTERMEDIATE_CSV}. Cannot run Phase 2.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸš¨ An unexpected error occurred during Phase 2: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4bYOSNaZpCq"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FINAL CELL: SHUTDOWN RUNTIME\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- All tasks complete. Shutting down the Colab runtime to save resources. ---\")\n",
        "print(\"You will be disconnected shortly.\")\n",
        "\n",
        "# This is the official command to disconnect and terminate the Colab runtime.\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}