{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/gemma-pipeline/blob/main/makale_sft_gemma_3_4b_test_pipeline_MMLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWNWiijSUDQ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZzL79PJXGGA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q-4GfMASUDQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCTzRCNlW7_-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastModel\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "MODEL_NAME = \"unsloth/gemma-3-4b-it\"\n",
        "\n",
        "DATASET_NAME = \"Yujivus/mmlu_grpo_test\"\n",
        "\n",
        "DATASET_CONFIG = \"pqa_labeled\"\n",
        "DATASET_SPLIT = \"train\" # Using the official test split\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # <--- PASTE YOUR KEY HERE\n",
        "JUDGE_MODEL = \"gpt-4.1-2025-04-14\"\n",
        "\n",
        "# File Paths for Outputs\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/gemma sft cevaplar\"\n",
        "FINAL_RESULTS_JSON = os.path.join(OUTPUT_DIR, \"final_evaluation_results_mmlu_base.json\")\n",
        "INTERMEDIATE_CSV = os.path.join(OUTPUT_DIR, \"intermediate_generated_answers_base_grpo.csv\")\n",
        "\n",
        "MAX_SAMPLES = 100\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded. Model: {MODEL_NAME}, Dataset: {DATASET_NAME}\")\n",
        "print(f\"Final output will be saved to: {FINAL_RESULTS_JSON}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VkH60sdYDMV"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 2: LOAD MODEL AND DATASET\n",
        "# ==============================================================================\n",
        "print(\"\\nLoading model and tokenizer with Unsloth...\")\n",
        "\n",
        "# --- Using your correct, full-precision model loading ---\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        ")\n",
        "print(\"âœ… Model loaded successfully in full precision.\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nLoading the 'train' split from dataset '{DATASET_NAME}'...\")\n",
        "test_data = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "\n",
        "if MAX_SAMPLES is not None:\n",
        "    print(f\"Selecting the first {MAX_SAMPLES} samples for this run.\")\n",
        "    test_data = test_data.select(range(MAX_SAMPLES))\n",
        "\n",
        "print(f\"âœ… Dataset ready. Using {len(test_data)} samples for evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q6QRbFIX9dN"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: PHASE 1 - GENERATE ANSWERS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 1: Generating Answers ---\")\n",
        "\n",
        "generated_results = []\n",
        "\n",
        "# Loop through the combined dataset\n",
        "for idx, item in enumerate(tqdm(test_data, desc=\"Generating Answers\")):\n",
        "    # CHANGED: Using the new column names from your MMLU dataset\n",
        "    question_text = item['question']\n",
        "    ground_truth_answer = item['answer']\n",
        "    subject = item['subject']\n",
        "\n",
        "    # CHANGED: The prompt is simpler as there is no separate context.\n",
        "    # The entire problem, including options, is in the 'question' field.\n",
        "    user_prompt = f\"Please answer the following multiple-choice question:\\n\\n{question_text}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the model's response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # The model's answer is everything after the initial prompt\n",
        "    model_full_answer = full_response.split(user_prompt)[-1].strip()\n",
        "\n",
        "    # CHANGED: Saving the relevant columns for this dataset\n",
        "    generated_results.append({\n",
        "        \"question_number\": idx,\n",
        "        \"question\": question_text,\n",
        "        \"subject\": subject,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"model_full_answer\": model_full_answer,\n",
        "    })\n",
        "\n",
        "# Save intermediate results to a CSV file\n",
        "df_answers = pd.DataFrame(generated_results)\n",
        "df_answers.to_csv(INTERMEDIATE_CSV, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Phase 1 Complete. {len(generated_results)} answers saved to {INTERMEDIATE_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_92PvLZ-b5a7"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 4: PHASE 2 - EVALUATE WITH LLM-AS-JUDGE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 2: Evaluating and Combining Results ---\")\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\") == \"\":\n",
        "    print(\"ðŸš¨ WARNING: OpenAI API key is not set. Skipping Phase 2.\")\n",
        "else:\n",
        "    try:\n",
        "        df_to_judge = pd.read_csv(INTERMEDIATE_CSV)\n",
        "        client = OpenAI()\n",
        "\n",
        "        final_results_list = []\n",
        "\n",
        "        # NEW: A judge prompt specifically for multiple-choice question evaluation\n",
        "        judge_system_prompt = \"\"\"You are an expert, impartial judge for evaluating an AI model's answer to a multiple-choice question.\n",
        "You will be given the original question (which includes the options), the ground-truth answer, and the model's full response.\n",
        "\n",
        "Your task is to determine if the model's answer is correct.\n",
        "- Focus on the selected option (e.g., 'A', 'B', 'C', or 'D').\n",
        "- The model might provide extra reasoning. The reasoning is not important for the correctness score, only the final chosen option.\n",
        "- Be lenient with formatting. For example, if the model says \"The correct answer is D\" and the ground truth is \"#### D. Vitamin B12\", this is correct. If the model says \"A\" and the ground truth is \"D\", this is incorrect.\n",
        "\n",
        "You MUST respond ONLY with a valid JSON object with the following structure:\n",
        "{\"correctness\": boolean, \"justification\": \"A brief explanation of your decision, noting the model's choice vs. the ground truth choice.\"}\n",
        "\"\"\"\n",
        "\n",
        "        for _, row in tqdm(df_to_judge.iterrows(), total=len(df_to_judge), desc=\"Judging Answers\"):\n",
        "            final_record = row.to_dict()\n",
        "\n",
        "            judge_user_prompt = f\"\"\"\n",
        "Please evaluate the following model output for a multiple-choice question.\n",
        "\n",
        "**Original Question:**\n",
        "{row['question']}\n",
        "\n",
        "**Ground Truth Answer:**\n",
        "{row['ground_truth_answer']}\n",
        "\n",
        "**Model's Full Response:**\n",
        "{row['model_full_answer']}\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=JUDGE_MODEL,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.0,\n",
        "                    response_format={\"type\": \"json_object\"}\n",
        "                )\n",
        "                judge_assessment = json.loads(response.choices[0].message.content)\n",
        "                final_record['judge_evaluation'] = judge_assessment\n",
        "            except Exception as e:\n",
        "                print(f\"Error judging question {row['question_number']}: {e}\")\n",
        "                final_record['judge_evaluation'] = {\"correctness\": \"error\", \"justification\": str(e)}\n",
        "\n",
        "            final_results_list.append(final_record)\n",
        "\n",
        "        # Save the final combined results to a single JSON file\n",
        "        with open(FINAL_RESULTS_JSON, 'w') as f:\n",
        "            json.dump(final_results_list, f, indent=4)\n",
        "        print(f\"\\nâœ… Phase 2 Complete. Final results saved to {FINAL_RESULTS_JSON}\")\n",
        "\n",
        "        # Final Summary\n",
        "        correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('correctness') is True)\n",
        "        total_judged = len(final_results_list)\n",
        "\n",
        "        if total_judged > 0:\n",
        "            accuracy = (correct_count / total_judged) * 100\n",
        "            print(\"\\n--- Final MMLU Results ---\")\n",
        "            print(f\"Answer Correctness: {accuracy:.2f}% ({correct_count}/{total_judged})\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ðŸš¨ ERROR: The intermediate answers file was not found at {INTERMEDIATE_CSV}. Cannot run Phase 2.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸš¨ An unexpected error occurred during Phase 2: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}