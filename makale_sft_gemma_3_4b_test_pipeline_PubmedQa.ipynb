{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/gemma-pipeline/blob/main/makale_sft_gemma_3_4b_test_pipeline_PubmedQa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y3PqVCZSUDP"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_OJLLijSUDP"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsrT0aOHSUDQ"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWNWiijSUDQ"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZzL79PJXGGA",
        "outputId": "e3435914-e84b-4d3c-c962-a46d9dca0175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q-4GfMASUDQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCTzRCNlW7_-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastModel\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "MODEL_NAME = \"Yujivus/gemma-3-1b-sft1\"\n",
        "\n",
        "\n",
        "DATASET_NAME = \"qiaojin/PubMedQA\"\n",
        "\n",
        "DATASET_CONFIG = \"pqa_labeled\"\n",
        "DATASET_SPLIT = \"train\" # Using the official test split\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # <--- PASTE YOUR KEY HERE\n",
        "JUDGE_MODEL = \"gpt-4.1-2025-04-14\" # \"gpt-4-turbo\" or \"gpt-4o\" are recommended\n",
        "\n",
        "# File Paths for Outputs\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/gemma sft cevaplar\"\n",
        "FINAL_RESULTS_JSON = os.path.join(OUTPUT_DIR, \"final_evaluation_results_4b_sft_model.json\")\n",
        "INTERMEDIATE_CSV = os.path.join(OUTPUT_DIR, \"/content/drive/MyDrive/gemma sft cevaplar/intermediate_generated_answers.csv\")\n",
        "\n",
        "\n",
        "MAX_SAMPLES = 100\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded. Model: {MODEL_NAME}, Dataset: {DATASET_NAME}\")\n",
        "print(f\"Final output will be saved to: {FINAL_RESULTS_JSON}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VkH60sdYDMV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: LOAD MODEL AND DATASET (REVISED)\n",
        "# ==============================================================================\n",
        "print(\"\\nLoading model and tokenizer with Unsloth...\")\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        ")\n",
        "print(\"‚úÖ Model loaded successfully.\")\n",
        "\n",
        "print(f\"\\nLoading PubMedQA dataset with config: '{DATASET_CONFIG}'...\")\n",
        "# CORRECTED: Pass the correct configuration name to load_dataset\n",
        "dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
        "test_data = dataset[DATASET_SPLIT]\n",
        "\n",
        "if MAX_SAMPLES is not None:\n",
        "    print(f\"Selecting the first {MAX_SAMPLES} samples for this run.\")\n",
        "    test_data = test_data.select(range(MAX_SAMPLES))\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded. Using {len(test_data)} samples from the '{DATASET_SPLIT}' split.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q6QRbFIX9dN"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: PHASE 1 - GENERATE ANSWERS (REVISED AND SIMPLIFIED)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 1: Generating Answers (without parsing) ---\")\n",
        "\n",
        "generated_results = []\n",
        "\n",
        "# Loop through the dataset with a progress bar\n",
        "for idx, item in enumerate(tqdm(test_data, desc=\"Generating Answers\")):\n",
        "    question = item['question']\n",
        "    context_text = \"\\n\".join(item['context']['contexts'])\n",
        "    ground_truth_answer = item['final_decision']\n",
        "    long_answer_context = item['long_answer']\n",
        "\n",
        "    user_prompt = f\"Context:\\n{context_text}\\n\\nQuestion:\\n{question}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}\n",
        "    ]\n",
        "\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.7, # This should have been 0.0\n",
        "        top_p=0.95,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    model_full_answer = full_response.split(text_input)[-1].strip()\n",
        "\n",
        "    generated_results.append({\n",
        "        \"question_number\": idx,\n",
        "        \"question\": question,\n",
        "        \"context\": context_text,\n",
        "        \"ground_truth_answer\": ground_truth_answer,\n",
        "        \"ground_truth_long_answer\": long_answer_context,\n",
        "        \"model_full_answer\": model_full_answer,\n",
        "    })\n",
        "\n",
        "# Save intermediate results to a CSV file\n",
        "df_answers = pd.DataFrame(generated_results)\n",
        "df_answers.to_csv(INTERMEDIATE_CSV, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Phase 1 Complete. {len(generated_results)} intermediate answers saved to {INTERMEDIATE_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_92PvLZ-b5a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: PHASE 2 - EVALUATE WITH LLM-AS-JUDGE AND COMBINE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Phase 2: Evaluating and Combining Results ---\")\n",
        "\n",
        "if os.environ.get(\"OPENAI_API_KEY\") == \"sk-...\":\n",
        "    print(\"üö® WARNING: OpenAI API key is not set. Skipping Phase 2.\")\n",
        "else:\n",
        "    try:\n",
        "        # Load the generated answers\n",
        "        df_to_judge = pd.read_csv(INTERMEDIATE_CSV)\n",
        "        client = OpenAI()\n",
        "\n",
        "        # REVISED: This list will hold the final combined data\n",
        "        final_results_list = []\n",
        "\n",
        "        judge_system_prompt = \"\"\"You are an expert, impartial judge for evaluating AI models in the medical domain.\n",
        "Your task is to assess a model's full response to a question based on a given context.\n",
        "\n",
        "First, you must carefully read the model's full response and determine its final, conclusive answer (e.g., 'yes', 'no', 'maybe').\n",
        "\n",
        "Then, evaluate based on two criteria:\n",
        "1.  `correctness`: Is the model's final answer correct when compared to the ground truth? (e.g., 'yes' vs 'yes' is correct). Be lenient with synonyms if applicable, but for yes/no questions, it should be precise.\n",
        "2.  `reasoning_correctness`: Did the model use the provided context and sound logic to arrive at its conclusion, or was the correct answer just a lucky guess? If the reasoning is irrelevant, nonsensical, or contradicts the context, this should be marked as incorrect.\n",
        "\n",
        "You MUST respond ONLY with a valid JSON object with the following structure:\n",
        "{\"extracted_answer\": \"The final answer you determined from the model's text\", \"correctness\": boolean, \"reasoning_correctness\": boolean, \"justification\": \"A brief explanation of your decision.\"}\n",
        "\"\"\"\n",
        "\n",
        "        for _, row in tqdm(df_to_judge.iterrows(), total=len(df_to_judge), desc=\"Judging Answers\"):\n",
        "            # REVISED: Start building the final record for this item\n",
        "            final_record = row.to_dict()\n",
        "\n",
        "            judge_user_prompt = f\"\"\"\n",
        "Please evaluate the following model output.\n",
        "\n",
        "**Original Question:**\n",
        "{row['question']}\n",
        "\n",
        "**Ground Truth Answer:**\n",
        "{row['ground_truth_answer']}\n",
        "\n",
        "**Model's Full Response (including reasoning):**\n",
        "{row['model_full_answer']}\n",
        "\"\"\"\n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=JUDGE_MODEL,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": judge_system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": judge_user_prompt}\n",
        "                    ],\n",
        "                    temperature=0.0, # Judge should be deterministic\n",
        "                    response_format={\"type\": \"json_object\"} # Enforce JSON output\n",
        "                )\n",
        "\n",
        "                judge_assessment = json.loads(response.choices[0].message.content)\n",
        "                # REVISED: Add the judge's assessment to the final record\n",
        "                final_record['judge_evaluation'] = judge_assessment\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error judging question {row['question_number']}: {e}\")\n",
        "                final_record['judge_evaluation'] = {\n",
        "                    \"correctness\": \"error\",\n",
        "                    \"reasoning_correctness\": \"error\",\n",
        "                    \"justification\": str(e)\n",
        "                }\n",
        "\n",
        "            # REVISED: Add the complete record to our final list\n",
        "            final_results_list.append(final_record)\n",
        "\n",
        "        # REVISED: Save the final combined list to a single JSON file\n",
        "        with open(FINAL_RESULTS_JSON, 'w') as f:\n",
        "            json.dump(final_results_list, f, indent=4)\n",
        "\n",
        "        print(f\"\\n‚úÖ Phase 2 Complete. Final combined results for {len(final_results_list)} items saved to {FINAL_RESULTS_JSON}\")\n",
        "\n",
        "        # Final Summary\n",
        "        correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('correctness') is True)\n",
        "        reasoning_correct_count = sum(1 for item in final_results_list if item.get('judge_evaluation', {}).get('reasoning_correctness') is True)\n",
        "        total_judged = len(final_results_list)\n",
        "\n",
        "        if total_judged > 0:\n",
        "            accuracy = (correct_count / total_judged) * 100\n",
        "            reasoning_accuracy = (reasoning_correct_count / total_judged) * 100\n",
        "            print(\"\\n--- Final Results ---\")\n",
        "            print(f\"Answer Correctness: {accuracy:.2f}% ({correct_count}/{total_judged})\")\n",
        "            print(f\"Reasoning Correctness: {reasoning_accuracy:.2f}% ({reasoning_correct_count}/{total_judged})\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"üö® ERROR: The intermediate answers file was not found at {INTERMEDIATE_CSV}. Cannot run Phase 2.\")\n",
        "    except Exception as e:\n",
        "        print(f\"üö® An unexpected error occurred during Phase 2: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}