{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/gemma-pipeline/blob/main/grpo_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpLEqWMKV6kB"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldri0atV6kE"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhQmFqp3V6kE"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSmmeDo9V6kF"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEig31SO-nMe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3mbPy2uV6kF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpzP00gbV6kG"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd2cpn1kaXRA"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rezoBIE1aXRA"
      },
      "source": [
        "Load up `Gemma 3 1B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"Yujivus/gemma-3-1b-sft1\",\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNuwc5sJ2pYK"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 32,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 64,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I0C3xy6uWW4n"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEibULDtlOMU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Yujivus/mmlu_grpo\", \"default\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIRsNJ_YitXl"
      },
      "outputs": [],
      "source": [
        "dataset[1][\"question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3yJTiFgtKYq"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5I3BCkViwuC"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "extract_hash_answer(dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHjiV3kGi8Y9"
      },
      "outputs": [],
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tkTF5Hmlhl-"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6MsfbGUtja0"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5X6oDNDn6Zj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\\\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVvrKUBEtoQD"
      },
      "outputs": [],
      "source": [
        "match_format.search(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    \"<SOLUTION>2</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8MPYPvvo1ri"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LlYVZjdpij9"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "        score += 0.2 if response.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.2 if response.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.2 if response.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.2 if response.count(solution_end)    == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqWply0z0DrP"
      },
      "outputs": [],
      "source": [
        "def extract_model_solution(completion_text: str) -> str | None:\n",
        "    \"\"\"Extracts text from between <SOLUTION> tags.\"\"\"\n",
        "    pattern = re.compile(f\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\", re.DOTALL)\n",
        "    match = pattern.search(completion_text)\n",
        "    if match: return match.group(1) # Return the raw text, including whitespace\n",
        "    return None\n",
        "\n",
        "def extract_all_option_pairs(question_text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Finds all multiple-choice option lines (e.g., \"A. ...\", \"B. ...\")\n",
        "    and returns them as a list of exact strings.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"^[A-Z][\\.\\)].*\", re.MULTILINE)\n",
        "    # Strip whitespace from each found option for clean matching.\n",
        "    return [opt.strip() for opt in pattern.findall(question_text)]\n",
        "    import re\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. HELPER FUNCTIONS (Unchanged)\n",
        "# ==============================================================================\n",
        "\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "def extract_model_solution(completion_text: str) -> str | None:\n",
        "    pattern = re.compile(f\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\", re.DOTALL)\n",
        "    match = pattern.search(completion_text)\n",
        "    if match: return match.group(1)\n",
        "    return None\n",
        "\n",
        "def extract_all_option_pairs(question_text: str) -> list[str]:\n",
        "    pattern = re.compile(r\"^[A-Z][\\.\\)].*\", re.MULTILINE)\n",
        "    return [opt.strip() for opt in pattern.findall(question_text)]\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. THE REWARD FUNCTION WITH REVISED SCORING\n",
        "# ==============================================================================\n",
        "def check_for_unique_pair_final_batched(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Looks for the presence of exact \"option-answer pair\" strings.\n",
        "    THIS VERSION IS BATCH-AWARE and uses the correct data access pattern.\n",
        "\n",
        "    - Rewards +5.0 if exactly one correct pair is found.\n",
        "    - Penalizes -2.0 if more than one pair is found (ambiguous).\n",
        "    - All other cases result in a neutral 0.0 score.\n",
        "    \"\"\"\n",
        "    all_questions_in_batch = kwargs.get(\"question\")\n",
        "    all_answers_in_batch = kwargs.get(\"answer\")\n",
        "\n",
        "    if not all_questions_in_batch or not all_answers_in_batch:\n",
        "        return [0.0] * len(completions)\n",
        "\n",
        "    scores = []\n",
        "    for i, completion in enumerate(completions):\n",
        "        question = all_questions_in_batch[i]\n",
        "        ground_truth_pair = all_answers_in_batch[i]\n",
        "\n",
        "        # --- THIS IS THE CORRECTED LINE ---\n",
        "        # `completion` is a list like [{\"content\": \"...\"}], so we access it with [0] then [\"content\"]\n",
        "        response_text = completion[0][\"content\"]\n",
        "\n",
        "        solution_text = extract_model_solution(response_text)\n",
        "\n",
        "        if solution_text is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        all_possible_pairs = extract_all_option_pairs(question)\n",
        "        found_pairs = [pair for pair in all_possible_pairs if pair in solution_text]\n",
        "\n",
        "        if len(found_pairs) > 1:\n",
        "            scores.append(-2.0)\n",
        "        elif len(found_pairs) == 1:\n",
        "            if found_pairs[0] == ground_truth_pair:\n",
        "                scores.append(5.0)\n",
        "            else:\n",
        "                scores.append(0.0)\n",
        "        else: # len(found_pairs) == 0\n",
        "            scores.append(0.0)\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1wlGH5sNqjO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. HELPER FUNCTIONS (Confirmed Safe)\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_model_solution(completion_text: str) -> str | None:\n",
        "    pattern = re.compile(f\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\", re.DOTALL)\n",
        "    match = pattern.search(completion_text)\n",
        "    if match: return match.group(1)\n",
        "    return None\n",
        "\n",
        "def extract_all_option_pairs(question_text: str) -> list[str]:\n",
        "    pattern = re.compile(r\"^[A-Z][\\.\\)].*\", re.MULTILINE)\n",
        "    return [opt.strip() for opt in pattern.findall(question_text)]\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. THE FINAL, BATCH-AWARE REWARD FUNCTION (Confirmed Correct)\n",
        "# ==============================================================================\n",
        "\n",
        "def check_for_unique_pair_final_batched(completions, **kwargs):\n",
        "    all_questions_in_batch = kwargs.get(\"question\")\n",
        "    all_answers_in_batch = kwargs.get(\"answer\")\n",
        "\n",
        "    if not all_questions_in_batch or not all_answers_in_batch:\n",
        "        return [0.0] * len(completions)\n",
        "\n",
        "    scores = []\n",
        "    for i, completion in enumerate(completions):\n",
        "        question = all_questions_in_batch[i]\n",
        "        ground_truth_pair = all_answers_in_batch[i]\n",
        "\n",
        "        response_text = completion[0][\"content\"]\n",
        "        solution_text = extract_model_solution(response_text)\n",
        "\n",
        "        if solution_text is None:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        all_possible_pairs = extract_all_option_pairs(question)\n",
        "        found_pairs = [pair for pair in all_possible_pairs if pair in solution_text]\n",
        "\n",
        "        if len(found_pairs) > 1:\n",
        "            scores.append(-2.0)\n",
        "        elif len(found_pairs) == 1:\n",
        "            if found_pairs[0] == ground_truth_pair:\n",
        "                scores.append(5.0)\n",
        "            else:\n",
        "                scores.append(0.0)\n",
        "        else:\n",
        "            scores.append(0.0)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TEST DATA AND THE UPDATED TEST RUNNER\n",
        "# ==============================================================================\n",
        "\n",
        "sample_data = {\n",
        "    \"question\": \"\"\"A 73-year-old woman...deficiency of which of the following vitamins?\n",
        "\n",
        "A. Vitamin B1 (thiamine)\n",
        "B. Vitamin B2 (riboflavin)\n",
        "C. Vitamin B6 (pyridoxine)\n",
        "D. Vitamin B12 (cyanocobalamin)\"\"\",\n",
        "    \"answer\": \"D. Vitamin B12 (cyanocobalamin)\"\n",
        "}\n",
        "\n",
        "final_test_cases = [\n",
        "    {\"description\": \"✅ [Correct] Simple case: Contains only the correct PAIR\", \"completion\": [{\"content\": f\"<SOLUTION>The correct choice is D. Vitamin B12 (cyanocobalamin).</SOLUTION>\"}], \"expected_score\": 5.0},\n",
        "    {\"description\": \"✅ [Correct] Your key example: Mentions other letters, but only one full PAIR\", \"completion\": [{\"content\": f\"<SOLUTION>The answer is not B or C. The correct answer is D. Vitamin B12 (cyanocobalamin).</SOLUTION>\"}], \"expected_score\": 5.0},\n",
        "    {\"description\": \"❌ [Penalized] Ambiguous: Contains two PAIRS\", \"completion\": [{\"content\": f\"<SOLUTION>It could be A. Vitamin B1 (thiamine). However, the better answer is D. Vitamin B12 (cyanocobalamin).</SOLUTION>\"}], \"expected_score\": -2.0},\n",
        "    {\"description\": \"➖ [Neutral] Incorrect: Contains only one, but incorrect, PAIR\", \"completion\": [{\"content\": f\"<SOLUTION>The answer is definitely A. Vitamin B1 (thiamine).</SOLUTION>\"}], \"expected_score\": 0.0},\n",
        "    {\"description\": \"➖ [Neutral] No PAIR: Only mentions the letter\", \"completion\": [{\"content\": f\"<SOLUTION>The answer is D.</SOLUTION>\"}], \"expected_score\": 0.0},\n",
        "]\n",
        "\n",
        "# --- THE CORRECTED TEST RUNNER ---\n",
        "print(\"🚀 Starting FINAL BATCH-AWARE Test Suite (Reward: +5)...\\n\" + \"=\"*60)\n",
        "all_passed = True\n",
        "for test in final_test_cases:\n",
        "    description = test[\"description\"]\n",
        "    completion_to_test = test[\"completion\"] # This is `[{\"content\": \"...\"}]`\n",
        "    expected = test[\"expected_score\"]\n",
        "\n",
        "    # THIS IS THE FIX: We now pass the data in the same list format the trainer uses.\n",
        "    actual_scores = check_for_unique_pair_final_batched(\n",
        "        completions=[completion_to_test],      # Simulates a batch: [[{\"content\":...}]]\n",
        "        question=[sample_data[\"question\"]],    # Simulates a batch: [\"The question...\"]\n",
        "        answer=[sample_data[\"answer\"]]         # Simulates a batch: [\"D. The answer...\"]\n",
        "    )\n",
        "    actual = actual_scores[0]\n",
        "\n",
        "    print(f\"🧪 TESTING: {description}\")\n",
        "    print(f\"   - Expected Score: {expected}\")\n",
        "    print(f\"   - Actual Score:   {actual}\")\n",
        "\n",
        "    if actual == expected:\n",
        "        print(\"   - STATUS: ✅ PASS\")\n",
        "    else:\n",
        "        print(f\"   - STATUS: ❌ FAIL\")\n",
        "        all_passed = False\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n🏁 Test Suite Finished.\")\n",
        "if all_passed:\n",
        "    print(\"🎉 All tests passed successfully!\")\n",
        "else:\n",
        "    print(\"🔥 Some tests failed. Please review the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "112fdfc1-ce5b-4c38-8d3e-ceb64e576ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 12\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = 512\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
        "    num_generations = 12, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "    num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    #max_steps = 50,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ii5K1Q7DxI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_for_unique_pair_final_batched,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"\"\"An expected side effect of creatine supplementation is:\n",
        "\n",
        "A. muscle weakness.\n",
        "B. gain in body mass.\n",
        "C. muscle cramps.\n",
        "D. loss of electrolytes.\"\"\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 2000, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "#model.save_pretrained(\"gemma-3-grpo1\")  # Local saving\n",
        "#tokenizer.save_pretrained(\"gemma-3\")\n",
        "model.push_to_hub(\"Yujivus/gemma-3_sft_grpo\", token = \"\") # Online saving\n",
        "tokenizer.push_to_hub(\"Yujivus/gemma-3_sft_grpo\", token = \"\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPTrTJ9-aRS5"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FINAL CELL: SHUTDOWN RUNTIME\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- All tasks complete. Shutting down the Colab runtime to save resources. ---\")\n",
        "print(\"You will be disconnected shortly.\")\n",
        "\n",
        "# This is the official command to disconnect and terminate the Colab runtime.\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}